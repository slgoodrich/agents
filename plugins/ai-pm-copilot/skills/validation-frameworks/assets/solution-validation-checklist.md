# Solution Validation Checklist

A step-by-step checklist for validating your solution before full development.

**Important**: Only use this after problem validation is complete. If you haven't validated the problem, return to the Problem Validation Canvas first.

---

## Pre-Validation: Solution Concept Definition

Before validating, clearly define what you're validating.

### [ ] Solution Concept Documented

**One-sentence solution**:
```
[Product name] helps [target users] [solve problem] by [key differentiator/approach].
```

**Example**:
"HourTracker helps freelancers track billable time accurately by automatically detecting project switches and suggesting time entries."

---

### [ ] Value Proposition Defined

**For [target user]**:


**Who [problem/need]**:


**Our solution [product/service]**:


**Provides [key benefit]**:


**Unlike [alternatives]**:


**We [key differentiator]**:


---

### [ ] Core Features Identified

**Must-have features** (MVP):
1.
2.
3.
4.
5.

**Nice-to-have features** (post-MVP):
1.
2.
3.

---

## Phase 1: Concept Validation

Validate that users understand and are interested in the solution concept.

### [ ] Method 1: Concept Description Test

**What**: Describe solution to target users without showing anything visual

**How**:
1. Read/show solution description
2. Ask: "In your own words, what does this do?"
3. Ask: "Who do you think this is for?"
4. Ask: "What problem does this solve?"

**Success criteria**:
- [ ] 7+ / 10 users correctly understand what it does
- [ ] 7+ / 10 users correctly identify the problem it solves

**Results**:
- Users tested: ____
- Understood correctly: ____
- Common misunderstandings:
  -
  -

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 2: Five-Second Test (First Impressions)

**What**: Show landing page or design for 5 seconds

**How**:
1. Show design for 5 seconds
2. Hide it
3. Ask: "What do you remember?"
4. Ask: "What do you think you can do with this?"
5. Ask: "What was your first impression?"

**Success criteria**:
- [ ] 6+ / 10 remember key value proposition
- [ ] 6+ / 10 understand what they can do

**Results**:
- Users tested: ____
- Remembered value prop: ____
- Most common first impression:

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 3: Interest/Demand Signal

**What**: Measure interest before building

**Options**:
- [ ] Landing page with email signup
- [ ] Pre-order or waitlist
- [ ] Fake door test (measure clicks on "coming soon" feature)

**Setup**:
- Traffic source: ________________
- Sample size target: ________________
- Test duration: ________________

**Success criteria**:
- [ ] Email signup rate > _____%
- [ ] Pre-order conversion > _____%
- [ ] OR _____ signups in _____ days

**Results**:
- Conversion rate: ______%
- Total signups: ______
- Cost per signup: $______

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 2: Usability Validation

Validate that users can actually use the solution to solve their problem.

### [ ] Method 4: Paper Prototype Test

**What**: Test workflow with sketches/wireframes

**How**:
1. Show paper sketches of key screens
2. Give user a realistic task
3. Ask them to "walk through" how they'd complete it
4. Note confusion, questions, wrong turns

**Tasks to test**:
1. Task: ________________
   - Success: ___ / ___ users completed
2. Task: ________________
   - Success: ___ / ___ users completed
3. Task: ________________
   - Success: ___ / ___ users completed

**Success criteria**:
- [ ] 7+ / 10 users complete core tasks
- [ ] Workflow makes sense to users
- [ ] No fundamental misunderstandings of UI

**Results**:
- Completion rate: ______%
- Major usability issues found:
  1.
  2.
  3.

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 5: Clickable Prototype Test

**What**: Test interactive prototype (Figma, InVision, etc.)

**How**:
1. Create clickable prototype of core workflow
2. Give users realistic tasks
3. Observe completion, time, errors
4. Ask follow-up questions

**Tool used**: ________________

**Tasks to test**:
1. Task: ________________
   - Completion: ___ / ___ users
   - Avg time: _____ minutes
2. Task: ________________
   - Completion: ___ / ___ users
   - Avg time: _____ minutes
3. Task: ________________
   - Completion: ___ / ___ users
   - Avg time: _____ minutes

**Success criteria**:
- [ ] 8+ / 10 users complete core workflow
- [ ] Average time < _____ minutes
- [ ] No critical usability issues

**Results**:
- Overall completion: ______%
- Critical issues: ______
- Moderate issues: ______

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 3: Value Validation

Validate that solution provides enough value that users would actually use/pay for it.

### [ ] Method 6: Concierge MVP

**What**: Manually deliver the outcome before building automation

**How**:
1. Recruit 5-10 users
2. Manually perform the service
3. Deliver results as if automated
4. Measure usage, satisfaction, retention

**Example**: Manually curate recommendations before building algorithm

**Concierge process**:
1.
2.
3.

**Success criteria**:
- [ ] 6+ / 10 users use it for 2+ weeks
- [ ] 7+ / 10 rate value as high
- [ ] 5+ / 10 say they'd pay for it

**Results**:
- 2-week retention: ______%
- Avg value rating (1-10): ______
- Would pay: ___ / ___ users

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 7: Wizard of Oz MVP

**What**: Build frontend, fake the backend

**How**:
1. Build user-facing interface
2. Manually process on backend
3. Users think it's automated
4. Measure actual usage behavior

**Example**: "AI" that's actually humans reviewing behind scenes

**What you're faking**:
-
-

**Success criteria**:
- [ ] Users use it naturally (don't realize it's manual)
- [ ] Engagement metrics meet targets
- [ ] Task completion rate > _____%

**Results**:
- Usage frequency: ________________
- Task completion: ______%
- Time to value: ______

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 4: Pricing Validation

Validate willingness to pay and appropriate price point.

### [ ] Method 8: Pricing Interviews

**What**: Explicit conversations about pricing

**How**:
1. After demonstrating value (concierge/prototype)
2. Ask: "What would be a fair price for this?"
3. Ask: "What would be so expensive you wouldn't consider it?"
4. Ask: "What would be so cheap you'd question the quality?"
5. Ask: "What do you currently pay for [similar category]?"

**Van Westendorp Price Sensitivity Meter**:
- Too cheap (quality concern): $_______
- Bargain: $_______
- Fair price: $_______
- Too expensive: $_______

**Results from _____ users**:
- Average "fair price": $_______
- Price range: $_______ - $_______

**Recommended pricing**:
- [ ] Freemium: Free + $_______ premium
- [ ] Subscription: $_______ / month
- [ ] One-time: $_______
- [ ] Usage-based: $_______ per _______

---

### [ ] Method 9: Pricing Page Test

**What**: Test different price points with real traffic

**How**:
1. Create pricing page(s) with different price points
2. Drive traffic from target audience
3. Measure conversion to signup/purchase intent
4. Analyze price sensitivity

**Variants tested**:
- Variant A: $_______  →  Conversion: ______%
- Variant B: $_______  →  Conversion: ______%
- Variant C: $_______  →  Conversion: ______%

**Success criteria**:
- [ ] Conversion rate > _____%
- [ ] Price identified that maximizes [revenue/conversions]

**Optimal price**: $_______

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 5: Market Validation

Validate that you can reach users and they'll choose you over alternatives.

### [ ] Method 10: Competitive Positioning Test

**What**: Validate differentiation in market context

**How**:
1. Show users your solution alongside competitors
2. Ask which they'd choose and why
3. Test messaging/positioning

**Competitors shown**:
1. ________________
2. ________________
3. ________________

**Results from _____ users**:
- Chose your solution: ___ / ___
- Reasons cited:
  1.
  2.
  3.

**Success criteria**:
- [ ] Your solution chosen by 40%+ of users
- [ ] Differentiation is clear and valued

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 11: Channel Validation

**What**: Validate you can acquire users cost-effectively

**How**:
1. Test customer acquisition channels
2. Measure cost per acquisition
3. Validate channel/message fit

**Channels tested**:
- Channel 1: ________________
  - Cost per signup: $_______
  - Conversion to activation: ______%
- Channel 2: ________________
  - Cost per signup: $_______
  - Conversion to activation: ______%

**Success criteria**:
- [ ] CAC < target of $_______
- [ ] At least 1 scalable channel identified

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 6: MVP Validation (Pre-Launch)

Final validation with functional MVP before public launch.

### [ ] Method 12: Private Beta

**What**: Limited release to early adopters

**How**:
1. Recruit 20-50 beta users
2. Full access to MVP
3. Measure usage, retention, feedback
4. Iterate based on learnings

**Beta scope**:
- Users: ______
- Duration: ______ weeks
- Features: ________________

**Success criteria**:
- [ ] Week 1 activation rate > _____%
- [ ] Week 2 retention > _____%
- [ ] Week 4 retention > _____%
- [ ] NPS or satisfaction > _____

**Results**:
- Activation rate: ______%
- Week 2 retention: ______%
- Week 4 retention: ______%
- NPS: ______
- Top feedback themes:
  1.
  2.
  3.

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Overall Solution Validation Assessment

### Validation Summary

**Concept Validation**: [ ] Pass [ ] Fail
- Users understand it
- Interest/demand validated

**Usability Validation**: [ ] Pass [ ] Fail
- Users can use it
- Workflows validated

**Value Validation**: [ ] Pass [ ] Fail
- Users find it valuable
- Would use repeatedly

**Pricing Validation**: [ ] Pass [ ] Fail
- Users will pay
- Price point identified

**Market Validation**: [ ] Pass [ ] Fail
- Can acquire users
- Competitive position validated

**MVP Validation**: [ ] Pass [ ] Fail
- Beta successful
- Ready for launch

---

### Key Learnings

**What we got right**:
1.
2.
3.

**What we got wrong**:
1.
2.
3.

**Biggest surprises**:
1.
2.
3.

---

### Launch Decision

**Overall validation strength**:
- [ ] Strong - Proceed to launch with confidence
- [ ] Moderate - Launch with caution, iterate quickly
- [ ] Weak - Significant iteration needed before launch

**Unvalidated assumptions remaining**:
1.
2.
3.

**Plan to validate after launch**:
1.
2.
3.

**Final decision**:
- [ ] Proceed to public launch
- [ ] Extended beta / soft launch
- [ ] Significant iteration needed
- [ ] Pivot solution approach
- [ ] Return to problem validation

---

## Post-Launch Validation

### Metrics to Track

**Acquisition**:
- [ ] CAC (Customer Acquisition Cost): $_______ (target: $_______)
- [ ] Conversion rate: _______% (target: ______%)
- [ ] Time to signup: _______ (target: _______)

**Activation**:
- [ ] Activation rate: _______% (target: ______%)
- [ ] Time to first value: _______ (target: _______)
- [ ] Completion of key actions: _______% (target: ______%)

**Retention**:
- [ ] Day 1: _______% (target: ______%)
- [ ] Day 7: _______% (target: ______%)
- [ ] Day 30: _______% (target: ______%)
- [ ] Day 90: _______% (target: ______%)

**Revenue** (if applicable):
- [ ] Conversion to paid: _______% (target: ______%)
- [ ] LTV: $_______ (target: $_______)
- [ ] Churn: _______% (target: ______%)

**Referral**:
- [ ] NPS: _______ (target: _______)
- [ ] Referral rate: _______% (target: ______%)

---

## Tips for Effective Solution Validation

### Do's:
- Test with real target users, not friends/family
- Test one variable at a time
- Set success criteria before running tests
- Be willing to hear you're wrong
- Iterate based on learnings
- Test progressively (cheap tests first, expensive tests last)

### Don'ts:
- Don't build before validating concept
- Don't skip usability testing
- Don't assume you know the right price
- Don't ignore competitor comparison
- Don't launch without beta testing
- Don't confuse enthusiasm with commitment

---

**Remember**: The goal isn't to validate that you're right. It's to learn what's true. Every "failure" is valuable learning that saves you from building the wrong thing.
