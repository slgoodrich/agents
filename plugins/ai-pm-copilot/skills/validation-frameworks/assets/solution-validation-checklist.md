# Solution Validation Checklist

A step-by-step checklist for validating your solution before full development.

**Important**: Only use this after problem validation is complete. If you haven't validated the problem, return to the Problem Validation Canvas first.

---

## Pre-Validation: Solution Concept Definition

Before validating, clearly define what you're validating.

### [ ] Solution Concept Documented

**One-sentence solution**:

```
[Product name] helps [target users] [solve problem] by [key differentiator/approach].
```

**Example**:
"HourTracker helps freelancers track billable time accurately by automatically detecting project switches and suggesting time entries."

---

### [ ] Value Proposition Defined

**For [target user]**:

**Who [problem/need]**:

**Our solution [product/service]**:

**Provides [key benefit]**:

**Unlike [alternatives]**:

**We [key differentiator]**:

---

### [ ] Core Features Identified

**Must-have features** (MVP):

1.
2.
3.
4.
5.

**Nice-to-have features** (post-MVP):

1.
2.
3.

---

## Phase 1: Concept Validation

Validate that users understand and are interested in the solution concept.

### [ ] Method 1: Concept Description Test

**What**: Describe solution to target users without showing anything visual

**How**:

1. Read/show solution description
2. Ask: "In your own words, what does this do?"
3. Ask: "Who do you think this is for?"
4. Ask: "What problem does this solve?"

**Success criteria**:

- [ ] 7+ / 10 users correctly understand what it does
- [ ] 7+ / 10 users correctly identify the problem it solves

**Results**:

- Users tested: \_\_\_\_
- Understood correctly: \_\_\_\_
- ## Common misunderstandings:
  -

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 2: Five-Second Test (First Impressions)

**What**: Show landing page or design for 5 seconds

**How**:

1. Show design for 5 seconds
2. Hide it
3. Ask: "What do you remember?"
4. Ask: "What do you think you can do with this?"
5. Ask: "What was your first impression?"

**Success criteria**:

- [ ] 6+ / 10 remember key value proposition
- [ ] 6+ / 10 understand what they can do

**Results**:

- Users tested: \_\_\_\_
- Remembered value prop: \_\_\_\_
- Most common first impression:

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 3: Interest/Demand Signal

**What**: Measure interest before building

**Options**:

- [ ] Landing page with email signup
- [ ] Pre-order or waitlist
- [ ] Fake door test (measure clicks on "coming soon" feature)

**Setup**:

- Traffic source: ******\_\_\_\_******
- Sample size target: ******\_\_\_\_******
- Test duration: ******\_\_\_\_******

**Success criteria**:

- [ ] Email signup rate > **\_**%
- [ ] Pre-order conversion > **\_**%
- [ ] OR **\_** signups in **\_** days

**Results**:

- Conversion rate: **\_\_**%
- Total signups: **\_\_**
- Cost per signup: $**\_\_**

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 2: Usability Validation

Validate that users can actually use the solution to solve their problem.

### [ ] Method 4: Paper Prototype Test

**What**: Test workflow with sketches/wireframes

**How**:

1. Show paper sketches of key screens
2. Give user a realistic task
3. Ask them to "walk through" how they'd complete it
4. Note confusion, questions, wrong turns

**Tasks to test**:

1. Task: ******\_\_\_\_******
   - Success: **_ / _** users completed
2. Task: ******\_\_\_\_******
   - Success: **_ / _** users completed
3. Task: ******\_\_\_\_******
   - Success: **_ / _** users completed

**Success criteria**:

- [ ] 7+ / 10 users complete core tasks
- [ ] Workflow makes sense to users
- [ ] No fundamental misunderstandings of UI

**Results**:

- Completion rate: **\_\_**%
- Major usability issues found:
  1.
  2.
  3.

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 5: Clickable Prototype Test

**What**: Test interactive prototype (Figma, InVision, etc.)

**How**:

1. Create clickable prototype of core workflow
2. Give users realistic tasks
3. Observe completion, time, errors
4. Ask follow-up questions

**Tool used**: ******\_\_\_\_******

**Tasks to test**:

1. Task: ******\_\_\_\_******
   - Completion: **_ / _** users
   - Avg time: **\_** minutes
2. Task: ******\_\_\_\_******
   - Completion: **_ / _** users
   - Avg time: **\_** minutes
3. Task: ******\_\_\_\_******
   - Completion: **_ / _** users
   - Avg time: **\_** minutes

**Success criteria**:

- [ ] 8+ / 10 users complete core workflow
- [ ] Average time < **\_** minutes
- [ ] No critical usability issues

**Results**:

- Overall completion: **\_\_**%
- Critical issues: **\_\_**
- Moderate issues: **\_\_**

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 3: Value Validation

Validate that solution provides enough value that users would actually use/pay for it.

### [ ] Method 6: Concierge MVP

**What**: Manually deliver the outcome before building automation

**How**:

1. Recruit 5-10 users
2. Manually perform the service
3. Deliver results as if automated
4. Measure usage, satisfaction, retention

**Example**: Manually curate recommendations before building algorithm

**Concierge process**:

1.
2.
3.

**Success criteria**:

- [ ] 6+ / 10 users use it for 2+ weeks
- [ ] 7+ / 10 rate value as high
- [ ] 5+ / 10 say they'd pay for it

**Results**:

- 2-week retention: **\_\_**%
- Avg value rating (1-10): **\_\_**
- Would pay: **_ / _** users

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 7: Wizard of Oz MVP

**What**: Build frontend, fake the backend

**How**:

1. Build user-facing interface
2. Manually process on backend
3. Users think it's automated
4. Measure actual usage behavior

**Example**: "AI" that's actually humans reviewing behind scenes

## **What you're faking**:

- **Success criteria**:

- [ ] Users use it naturally (don't realize it's manual)
- [ ] Engagement metrics meet targets
- [ ] Task completion rate > **\_**%

**Results**:

- Usage frequency: ******\_\_\_\_******
- Task completion: **\_\_**%
- Time to value: **\_\_**

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 4: Pricing Validation

Validate willingness to pay and appropriate price point.

### [ ] Method 8: Pricing Interviews

**What**: Explicit conversations about pricing

**How**:

1. After demonstrating value (concierge/prototype)
2. Ask: "What would be a fair price for this?"
3. Ask: "What would be so expensive you wouldn't consider it?"
4. Ask: "What would be so cheap you'd question the quality?"
5. Ask: "What do you currently pay for [similar category]?"

**Van Westendorp Price Sensitivity Meter**:

- Too cheap (quality concern): $**\_\_\_**
- Bargain: $**\_\_\_**
- Fair price: $**\_\_\_**
- Too expensive: $**\_\_\_**

**Results from **\_** users**:

- Average "fair price": $**\_\_\_**
- Price range: $**\_\_\_** - $**\_\_\_**

**Recommended pricing**:

- [ ] Freemium: Free + $**\_\_\_** premium
- [ ] Subscription: $**\_\_\_** / month
- [ ] One-time: $**\_\_\_**
- [ ] Usage-based: $**\_\_\_** per **\_\_\_**

---

### [ ] Method 9: Pricing Page Test

**What**: Test different price points with real traffic

**How**:

1. Create pricing page(s) with different price points
2. Drive traffic from target audience
3. Measure conversion to signup/purchase intent
4. Analyze price sensitivity

**Variants tested**:

- Variant A: $**\_\_\_** → Conversion: **\_\_**%
- Variant B: $**\_\_\_** → Conversion: **\_\_**%
- Variant C: $**\_\_\_** → Conversion: **\_\_**%

**Success criteria**:

- [ ] Conversion rate > **\_**%
- [ ] Price identified that maximizes [revenue/conversions]

**Optimal price**: $**\_\_\_**

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 5: Market Validation

Validate that you can reach users and they'll choose you over alternatives.

### [ ] Method 10: Competitive Positioning Test

**What**: Validate differentiation in market context

**How**:

1. Show users your solution alongside competitors
2. Ask which they'd choose and why
3. Test messaging/positioning

**Competitors shown**:

1. ***
2. ***
3. ***

**Results from **\_** users**:

- Chose your solution: **_ / _**
- Reasons cited:
  1.
  2.
  3.

**Success criteria**:

- [ ] Your solution chosen by 40%+ of users
- [ ] Differentiation is clear and valued

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

### [ ] Method 11: Channel Validation

**What**: Validate you can acquire users cost-effectively

**How**:

1. Test customer acquisition channels
2. Measure cost per acquisition
3. Validate channel/message fit

**Channels tested**:

- Channel 1: ******\_\_\_\_******
  - Cost per signup: $**\_\_\_**
  - Conversion to activation: **\_\_**%
- Channel 2: ******\_\_\_\_******
  - Cost per signup: $**\_\_\_**
  - Conversion to activation: **\_\_**%

**Success criteria**:

- [ ] CAC < target of $**\_\_\_**
- [ ] At least 1 scalable channel identified

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Phase 6: MVP Validation (Pre-Launch)

Final validation with functional MVP before public launch.

### [ ] Method 12: Private Beta

**What**: Limited release to early adopters

**How**:

1. Recruit 20-50 beta users
2. Full access to MVP
3. Measure usage, retention, feedback
4. Iterate based on learnings

**Beta scope**:

- Users: **\_\_**
- Duration: **\_\_** weeks
- Features: ******\_\_\_\_******

**Success criteria**:

- [ ] Week 1 activation rate > **\_**%
- [ ] Week 2 retention > **\_**%
- [ ] Week 4 retention > **\_**%
- [ ] NPS or satisfaction > **\_**

**Results**:

- Activation rate: **\_\_**%
- Week 2 retention: **\_\_**%
- Week 4 retention: **\_\_**%
- NPS: **\_\_**
- Top feedback themes:
  1.
  2.
  3.

**Status**: [ ] Pass [ ] Fail [ ] Needs iteration

---

## Overall Solution Validation Assessment

### Validation Summary

**Concept Validation**: [ ] Pass [ ] Fail

- Users understand it
- Interest/demand validated

**Usability Validation**: [ ] Pass [ ] Fail

- Users can use it
- Workflows validated

**Value Validation**: [ ] Pass [ ] Fail

- Users find it valuable
- Would use repeatedly

**Pricing Validation**: [ ] Pass [ ] Fail

- Users will pay
- Price point identified

**Market Validation**: [ ] Pass [ ] Fail

- Can acquire users
- Competitive position validated

**MVP Validation**: [ ] Pass [ ] Fail

- Beta successful
- Ready for launch

---

### Key Learnings

**What we got right**:

1.
2.
3.

**What we got wrong**:

1.
2.
3.

**Biggest surprises**:

1.
2.
3.

---

### Launch Decision

**Overall validation strength**:

- [ ] Strong - Proceed to launch with confidence
- [ ] Moderate - Launch with caution, iterate quickly
- [ ] Weak - Significant iteration needed before launch

**Unvalidated assumptions remaining**:

1.
2.
3.

**Plan to validate after launch**:

1.
2.
3.

**Final decision**:

- [ ] Proceed to public launch
- [ ] Extended beta / soft launch
- [ ] Significant iteration needed
- [ ] Pivot solution approach
- [ ] Return to problem validation

---

## Post-Launch Validation

### Metrics to Track

**Acquisition**:

- [ ] CAC (Customer Acquisition Cost): $**\_\_\_** (target: $**\_\_\_**)
- [ ] Conversion rate: **\_\_\_**% (target: **\_\_**%)
- [ ] Time to signup: **\_\_\_** (target: **\_\_\_**)

**Activation**:

- [ ] Activation rate: **\_\_\_**% (target: **\_\_**%)
- [ ] Time to first value: **\_\_\_** (target: **\_\_\_**)
- [ ] Completion of key actions: **\_\_\_**% (target: **\_\_**%)

**Retention**:

- [ ] Day 1: **\_\_\_**% (target: **\_\_**%)
- [ ] Day 7: **\_\_\_**% (target: **\_\_**%)
- [ ] Day 30: **\_\_\_**% (target: **\_\_**%)
- [ ] Day 90: **\_\_\_**% (target: **\_\_**%)

**Revenue** (if applicable):

- [ ] Conversion to paid: **\_\_\_**% (target: **\_\_**%)
- [ ] LTV: $**\_\_\_** (target: $**\_\_\_**)
- [ ] Churn: **\_\_\_**% (target: **\_\_**%)

**Referral**:

- [ ] NPS: **\_\_\_** (target: **\_\_\_**)
- [ ] Referral rate: **\_\_\_**% (target: **\_\_**%)

---

## Tips for Effective Solution Validation

### Do's:

- Test with real target users, not friends/family
- Test one variable at a time
- Set success criteria before running tests
- Be willing to hear you're wrong
- Iterate based on learnings
- Test progressively (cheap tests first, expensive tests last)

### Don'ts:

- Don't build before validating concept
- Don't skip usability testing
- Don't assume you know the right price
- Don't ignore competitor comparison
- Don't launch without beta testing
- Don't confuse enthusiasm with commitment

---

**Remember**: The goal isn't to validate that you're right. It's to learn what's true. Every "failure" is valuable learning that saves you from building the wrong thing.
