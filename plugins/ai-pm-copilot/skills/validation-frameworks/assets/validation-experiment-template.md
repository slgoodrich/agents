# Validation Experiment Template

Use this template to design rigorous experiments that test your product assumptions.

---

## Experiment Overview

**Experiment Name**: ________________

**Date**: ________________

**Owner**: ________________

**Status**: [ ] Planned [ ] Running [ ] Complete [ ] Abandoned

---

## 1. Assumption to Test

### The Assumption

**Write assumption as a testable hypothesis:**

```
We believe that [target users] will [specific behavior]
when [context/situation] because [reason].
```

**Example**:
"We believe that freelancers will manually log their time entries daily when using a simple mobile interface because they want accurate billing but current tools are too complex."

**Our assumption**:


---

### Why This Assumption Matters

**Type of assumption**:
- [ ] Desirability - Will users want this?
- [ ] Usability - Can users use this?
- [ ] Feasibility - Can we build this?
- [ ] Viability - Should we build this?

**Impact if assumption is wrong**:
- [ ] Critical - Entire product depends on this
- [ ] High - Major pivot would be needed
- [ ] Medium - Significant changes required
- [ ] Low - Minor adjustment needed

**Current confidence level**:
- [ ] High (> 70% confident) - Why test? ________________
- [ ] Medium (30-70% confident)
- [ ] Low (< 30% confident)
- [ ] Pure guess

**Why we're testing this now**:
-


---

## 2. Experiment Design

### What We'll Test

**Specific behavior or outcome to observe**:


**What success looks like** (be specific and measurable):
```
If [X%] of [user group] [do specific action] within [timeframe],
then we will consider the assumption validated.
```

**Example**:
"If 60% of freelancers log time at least 5 days per week within their first 2 weeks of using the app, then we will consider the assumption validated."

**Our success criteria**:


---

### Three-Point Scale

Define three outcome levels before running experiment:

**Strong Success** (Green light):
- Metric: ________________
- What this means: Proceed confidently
- Next step if we hit this: ________________

**Moderate Success** (Yellow light):
- Metric: ________________
- What this means: Promising but needs refinement
- Next step if we hit this: ________________

**Failure** (Red light):
- Metric: ________________
- What this means: Assumption is wrong
- Next step if we hit this: ________________

---

### Method

**How we'll test this**:
- [ ] Customer interviews
- [ ] Prototype test
- [ ] Landing page test
- [ ] Concierge MVP
- [ ] Wizard of Oz MVP
- [ ] Private beta
- [ ] A/B test
- [ ] Survey
- [ ] Data analysis
- [ ] Other: ________________

**Why this method**:
- Fastest way to learn: [ ] Yes [ ] No
- Cheapest way to learn: [ ] Yes [ ] No
- Most accurate: [ ] Yes [ ] No
- Selected because: ________________

---

### Participants/Sample

**Who we'll test with**:
- Target segment: ________________
- Sample size: ________ users
- Why this sample size: ________________

**Recruitment**:
- How we'll find them: ________________
- Screening criteria:
  1.
  2.
  3.
- Incentive (if any): ________________

**Exclusions** (who should NOT be in test):
-
-

---

## 3. Experiment Setup

### What We'll Build/Prepare

**Minimum viable test** (what's the least we can build?):
-
-
-

**Assets needed**:
- [ ] Prototype / mockup
- [ ] Landing page
- [ ] Survey questions
- [ ] Interview script
- [ ] Tracking/analytics
- [ ] Other: ________________

**Effort estimate**: ________ hours/days

**Cost estimate**: $_________

---

### Measurements

**Primary metric** (main success indicator):
- Metric: ________________
- How we'll measure: ________________
- Target: ________ (from success criteria above)

**Secondary metrics** (supporting evidence):
1. Metric: ________________
   - Target: ________
2. Metric: ________________
   - Target: ________

**Qualitative data to collect**:
- [ ] User quotes
- [ ] Observations
- [ ] Follow-up questions
- [ ] Other: ________________

---

### Timeline

**Preparation**:
- Start: ________
- End: ________
- Key tasks:
  1.
  2.
  3.

**Execution**:
- Start: ________
- End: ________
- Duration: ________ days/weeks

**Analysis**:
- Complete by: ________

**Decision deadline**:
- When: ________

---

## 4. Risks and Mitigation

### Potential Issues

**What could go wrong?**

**Risk 1**: ________________
- Likelihood: [ ] High [ ] Medium [ ] Low
- Impact: [ ] High [ ] Medium [ ] Low
- Mitigation: ________________

**Risk 2**: ________________
- Likelihood: [ ] High [ ] Medium [ ] Low
- Impact: [ ] High [ ] Medium [ ] Low
- Mitigation: ________________

**Risk 3**: ________________
- Likelihood: [ ] High [ ] Medium [ ] Low
- Impact: [ ] High [ ] Medium [ ] Low
- Mitigation: ________________

---

### Bias Check

**How might we accidentally bias this experiment?**

- [ ] Leading questions in interviews
- [ ] Testing with wrong audience
- [ ] Making test too easy/hard
- [ ] Confirmation bias in analysis
- [ ] Observer effect changing behavior
- [ ] Other: ________________

**Mitigation**:
-
-

---

## 5. Execution

### Pre-Flight Checklist

Before launching experiment:

- [ ] Success criteria defined and documented
- [ ] Sample size planned
- [ ] Participants recruited or plan in place
- [ ] Assets built
- [ ] Tracking/measurement implemented
- [ ] Team aligned on what we're learning
- [ ] Bias mitigation in place
- [ ] Pilot test complete (if applicable)

---

### During Experiment

**Log key events and observations:**

**Day/Week 1**:
- Date: ________
- Observations:
  -
  -
- Early data:
  -

**Day/Week 2**:
- Date: ________
- Observations:
  -
  -
- Data so far:
  -

**Midpoint check** (if long-running):
- On track? [ ] Yes [ ] No
- Any adjustments needed? ________________

---

### Unexpected Learnings

**Surprises during experiment**:
1.
2.
3.

**New questions raised**:
1.
2.
3.

---

## 6. Results

### Quantitative Results

**Primary metric result**:
- Measured: ________
- Target: ________
- Result: [ ] Exceeded [ ] Met [ ] Missed

**Secondary metrics**:
1. ________________: ________ (target: ________)
2. ________________: ________ (target: ________)

**Statistical significance** (if applicable):
- Sample size: ________
- Confidence level: ________%
- Statistically significant? [ ] Yes [ ] No

---

### Qualitative Results

**Key themes from interviews/feedback**:
1.
2.
3.

**Representative quotes**:
1. "________________________________________"
2. "________________________________________"
3. "________________________________________"

**Observed behaviors**:
-
-

---

### Outcome Assessment

**Which outcome did we hit?**
- [ ] Strong Success (Green) - Assumption strongly validated
- [ ] Moderate Success (Yellow) - Assumption partially validated
- [ ] Failure (Red) - Assumption invalidated

**Confidence in results**:
- [ ] High - Strong evidence, clear signal
- [ ] Medium - Evidence is promising but not conclusive
- [ ] Low - Results are ambiguous or confounded

---

## 7. Learnings

### What We Learned

**Was the assumption correct?**
- [ ] Yes, confirmed
- [ ] Partially - needs refinement
- [ ] No, invalidated

**What we learned about the assumption**:
-
-
-

**What we learned about our users**:
-
-
-

**What we learned about our product**:
-
-
-

---

### What We Got Wrong

**Initial assumptions that were incorrect**:
1.
2.
3.

**Surprises**:
1.
2.
3.

**New insights**:
1.
2.
3.

---

## 8. Decision & Next Steps

### Decision

Based on results, we will:

- [ ] **Proceed** - Assumption validated, move forward
  - Next step: ________________
  - Timeline: ________________

- [ ] **Iterate** - Refine and test again
  - What we'll change: ________________
  - Next experiment: ________________
  - Timeline: ________________

- [ ] **Pivot** - Assumption wrong, change direction
  - What we'll pivot to: ________________
  - Next steps: ________________

- [ ] **Abandon** - Not viable, stop work on this
  - Reason: ________________
  - Alternative paths: ________________

---

### Follow-Up Experiments

**New assumptions to test** (based on learnings):
1.
2.
3.

**Next experiment**:
- Assumption: ________________
- Method: ________________
- Timeline: ________________

---

### Clear Communication

**Key message**:
```
We tested [assumption] by [method] with [sample].
We learned that [key learning].
Therefore, we will [decision/next step].
```

**Who to inform**:
- [ ] Team
- [ ] Leadership
- [ ] Investors
- [ ] Other: ________________

**Communication method**:
- [ ] Slack/email update
- [ ] Presentation
- [ ] Written report
- [ ] Meeting

---

## 9. Documentation

### Artifacts

**Link to supporting materials**:
- Raw data: ________________
- Interview notes: ________________
- Prototype: ________________
- Screenshots: ________________
- Analytics dashboard: ________________

**Files saved in**: ________________

---

### Knowledge Sharing

**Key insights to share**:
1.
2.
3.

**Added to knowledge base**: [ ] Yes [ ] No
**Location**: ________________

**Team debrief**:
- Date: ________
- Attendees: ________________
- Key discussion points:
  -
  -

---

## 10. Reflection

### Process Retrospective

**What went well?**
1.
2.
3.

**What could be improved?**
1.
2.
3.

**For next experiment, we should**:
1.
2.
3.

---

### Cost-Benefit Analysis

**Time invested**: ________ hours/days
**Money invested**: $_________

**Value of learning**:
- [ ] Extremely valuable - Saved us from major mistake
- [ ] Very valuable - Validated important assumption
- [ ] Moderately valuable - Helpful but expected
- [ ] Somewhat valuable - Nice to know
- [ ] Not valuable - Could have assumed this

**Would you run this experiment again?**
- [ ] Yes, exactly as-is
- [ ] Yes, but with modifications: ________________
- [ ] No, would use different method: ________________
- [ ] No, not worth it

---

## Template Usage Tips

### Before Starting

**Use this template when**:
- Testing critical assumptions
- Making important product decisions
- Unsure about user behavior
- Considering significant investment

**Don't use when**:
- Answer is easily observable
- Risk of being wrong is very low
- Learning isn't worth time/cost

---

### During Experiment

**Best practices**:
- Define success criteria before seeing results
- Actively seek disconfirming evidence
- Document everything (you'll forget details)
- Don't change experiment mid-flight
- Separate data collection from analysis

---

### After Experiment

**Share learnings**:
- Failed experiments are valuable
- Document "why" not just "what"
- Update team's shared assumptions
- Build institutional knowledge

---

## Example: Completed Experiment

**Experiment Name**: Time Tracking Auto-Suggest Validation

**Assumption**: "Freelancers will accept 80%+ of auto-suggested time entries when switching projects"

**Method**: Wizard of Oz (manual suggestions disguised as automated)

**Sample**: 10 freelancers, 2 weeks

**Success Criteria**:
- Strong: > 80% acceptance rate
- Moderate: 60-80% acceptance
- Failure: < 60% acceptance

**Result**: 45% acceptance rate (Failure)

**Learning**: Users want control, not automation. They'll use suggestions as reminders but want to edit. Changed approach to "prompt + easy edit" rather than "auto-suggest + accept/reject"

**Decision**: Iterate - Change UX to treat as prompts, not automation. Test revised approach.

---

**Remember**: A well-designed experiment teaches you something valuable whether it succeeds or fails. The only failure is not learning.
