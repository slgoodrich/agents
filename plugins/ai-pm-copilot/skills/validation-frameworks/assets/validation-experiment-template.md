# Validation Experiment Template

Use this template to design rigorous experiments that test your product assumptions.

---

## Experiment Overview

**Experiment Name**: ******\_\_\_\_******

**Date**: ******\_\_\_\_******

**Owner**: ******\_\_\_\_******

**Status**: [ ] Planned [ ] Running [ ] Complete [ ] Abandoned

---

## 1. Assumption to Test

### The Assumption

**Write assumption as a testable hypothesis:**

```
We believe that [target users] will [specific behavior]
when [context/situation] because [reason].
```

**Example**:
"We believe that freelancers will manually log their time entries daily when using a simple mobile interface because they want accurate billing but current tools are too complex."

**Our assumption**:

---

### Why This Assumption Matters

**Type of assumption**:

- [ ] Desirability - Will users want this?
- [ ] Usability - Can users use this?
- [ ] Feasibility - Can we build this?
- [ ] Viability - Should we build this?

**Impact if assumption is wrong**:

- [ ] Critical - Entire product depends on this
- [ ] High - Major pivot would be needed
- [ ] Medium - Significant changes required
- [ ] Low - Minor adjustment needed

**Current confidence level**:

- [ ] High (> 70% confident) - Why test? ******\_\_\_\_******
- [ ] Medium (30-70% confident)
- [ ] Low (< 30% confident)
- [ ] Pure guess

## **Why we're testing this now**:

---

## 2. Experiment Design

### What We'll Test

**Specific behavior or outcome to observe**:

**What success looks like** (be specific and measurable):

```
If [X%] of [user group] [do specific action] within [timeframe],
then we will consider the assumption validated.
```

**Example**:
"If 60% of freelancers log time at least 5 days per week within their first 2 weeks of using the app, then we will consider the assumption validated."

**Our success criteria**:

---

### Three-Point Scale

Define three outcome levels before running experiment:

**Strong Success** (Green light):

- Metric: ******\_\_\_\_******
- What this means: Proceed confidently
- Next step if we hit this: ******\_\_\_\_******

**Moderate Success** (Yellow light):

- Metric: ******\_\_\_\_******
- What this means: Promising but needs refinement
- Next step if we hit this: ******\_\_\_\_******

**Failure** (Red light):

- Metric: ******\_\_\_\_******
- What this means: Assumption is wrong
- Next step if we hit this: ******\_\_\_\_******

---

### Method

**How we'll test this**:

- [ ] Customer interviews
- [ ] Prototype test
- [ ] Landing page test
- [ ] Concierge MVP
- [ ] Wizard of Oz MVP
- [ ] Private beta
- [ ] A/B test
- [ ] Survey
- [ ] Data analysis
- [ ] Other: ******\_\_\_\_******

**Why this method**:

- Fastest way to learn: [ ] Yes [ ] No
- Cheapest way to learn: [ ] Yes [ ] No
- Most accurate: [ ] Yes [ ] No
- Selected because: ******\_\_\_\_******

---

### Participants/Sample

**Who we'll test with**:

- Target segment: ******\_\_\_\_******
- Sample size: **\_\_\_\_** users
- Why this sample size: ******\_\_\_\_******

**Recruitment**:

- How we'll find them: ******\_\_\_\_******
- Screening criteria:
  1.
  2.
  3.
- Incentive (if any): ******\_\_\_\_******

## **Exclusions** (who should NOT be in test):

- ***

## 3. Experiment Setup

### What We'll Build/Prepare

## **Minimum viable test** (what's the least we can build?):

-
- **Assets needed**:

- [ ] Prototype / mockup
- [ ] Landing page
- [ ] Survey questions
- [ ] Interview script
- [ ] Tracking/analytics
- [ ] Other: ******\_\_\_\_******

**Effort estimate**: **\_\_\_\_** hours/days

**Cost estimate**: $****\_****

---

### Measurements

**Primary metric** (main success indicator):

- Metric: ******\_\_\_\_******
- How we'll measure: ******\_\_\_\_******
- Target: **\_\_\_\_** (from success criteria above)

**Secondary metrics** (supporting evidence):

1. Metric: ******\_\_\_\_******
   - Target: **\_\_\_\_**
2. Metric: ******\_\_\_\_******
   - Target: **\_\_\_\_**

**Qualitative data to collect**:

- [ ] User quotes
- [ ] Observations
- [ ] Follow-up questions
- [ ] Other: ******\_\_\_\_******

---

### Timeline

**Preparation**:

- Start: **\_\_\_\_**
- End: **\_\_\_\_**
- Key tasks:
  1.
  2.
  3.

**Execution**:

- Start: **\_\_\_\_**
- End: **\_\_\_\_**
- Duration: **\_\_\_\_** days/weeks

**Analysis**:

- Complete by: **\_\_\_\_**

**Decision deadline**:

- When: **\_\_\_\_**

---

## 4. Risks and Mitigation

### Potential Issues

**What could go wrong?**

**Risk 1**: ******\_\_\_\_******

- Likelihood: [ ] High [ ] Medium [ ] Low
- Impact: [ ] High [ ] Medium [ ] Low
- Mitigation: ******\_\_\_\_******

**Risk 2**: ******\_\_\_\_******

- Likelihood: [ ] High [ ] Medium [ ] Low
- Impact: [ ] High [ ] Medium [ ] Low
- Mitigation: ******\_\_\_\_******

**Risk 3**: ******\_\_\_\_******

- Likelihood: [ ] High [ ] Medium [ ] Low
- Impact: [ ] High [ ] Medium [ ] Low
- Mitigation: ******\_\_\_\_******

---

### Bias Check

**How might we accidentally bias this experiment?**

- [ ] Leading questions in interviews
- [ ] Testing with wrong audience
- [ ] Making test too easy/hard
- [ ] Confirmation bias in analysis
- [ ] Observer effect changing behavior
- [ ] Other: ******\_\_\_\_******

## **Mitigation**:

- ***

## 5. Execution

### Pre-Flight Checklist

Before launching experiment:

- [ ] Success criteria defined and documented
- [ ] Sample size planned
- [ ] Participants recruited or plan in place
- [ ] Assets built
- [ ] Tracking/measurement implemented
- [ ] Team aligned on what we're learning
- [ ] Bias mitigation in place
- [ ] Pilot test complete (if applicable)

---

### During Experiment

**Log key events and observations:**

**Day/Week 1**:

- Date: **\_\_\_\_**
- ## Observations:
  -
- ## Early data:

**Day/Week 2**:

- Date: **\_\_\_\_**
- ## Observations:
  -
- ## Data so far:

**Midpoint check** (if long-running):

- On track? [ ] Yes [ ] No
- Any adjustments needed? ******\_\_\_\_******

---

### Unexpected Learnings

**Surprises during experiment**:

1.
2.
3.

**New questions raised**:

1.
2.
3.

---

## 6. Results

### Quantitative Results

**Primary metric result**:

- Measured: **\_\_\_\_**
- Target: **\_\_\_\_**
- Result: [ ] Exceeded [ ] Met [ ] Missed

**Secondary metrics**:

1. ******\_\_\_\_******: **\_\_\_\_** (target: **\_\_\_\_**)
2. ******\_\_\_\_******: **\_\_\_\_** (target: **\_\_\_\_**)

**Statistical significance** (if applicable):

- Sample size: **\_\_\_\_**
- Confidence level: **\_\_\_\_**%
- Statistically significant? [ ] Yes [ ] No

---

### Qualitative Results

**Key themes from interviews/feedback**:

1.
2.
3.

**Representative quotes**:

1. "******************\_\_\_\_******************"
2. "******************\_\_\_\_******************"
3. "******************\_\_\_\_******************"

## **Observed behaviors**:

- ***

### Outcome Assessment

**Which outcome did we hit?**

- [ ] Strong Success (Green) - Assumption strongly validated
- [ ] Moderate Success (Yellow) - Assumption partially validated
- [ ] Failure (Red) - Assumption invalidated

**Confidence in results**:

- [ ] High - Strong evidence, clear signal
- [ ] Medium - Evidence is promising but not conclusive
- [ ] Low - Results are ambiguous or confounded

---

## 7. Learnings

### What We Learned

**Was the assumption correct?**

- [ ] Yes, confirmed
- [ ] Partially - needs refinement
- [ ] No, invalidated

## **What we learned about the assumption**:

-
- **What we learned about our users**:

-
-
- **What we learned about our product**:

-
-
- ***

### What We Got Wrong

**Initial assumptions that were incorrect**:

1.
2.
3.

**Surprises**:

1.
2.
3.

**New insights**:

1.
2.
3.

---

## 8. Decision & Next Steps

### Decision

Based on results, we will:

- [ ] **Proceed** - Assumption validated, move forward
  - Next step: ******\_\_\_\_******
  - Timeline: ******\_\_\_\_******

- [ ] **Iterate** - Refine and test again
  - What we'll change: ******\_\_\_\_******
  - Next experiment: ******\_\_\_\_******
  - Timeline: ******\_\_\_\_******

- [ ] **Pivot** - Assumption wrong, change direction
  - What we'll pivot to: ******\_\_\_\_******
  - Next steps: ******\_\_\_\_******

- [ ] **Abandon** - Not viable, stop work on this
  - Reason: ******\_\_\_\_******
  - Alternative paths: ******\_\_\_\_******

---

### Follow-Up Experiments

**New assumptions to test** (based on learnings):

1.
2.
3.

**Next experiment**:

- Assumption: ******\_\_\_\_******
- Method: ******\_\_\_\_******
- Timeline: ******\_\_\_\_******

---

### Clear Communication

**Key message**:

```
We tested [assumption] by [method] with [sample].
We learned that [key learning].
Therefore, we will [decision/next step].
```

**Who to inform**:

- [ ] Team
- [ ] Leadership
- [ ] Investors
- [ ] Other: ******\_\_\_\_******

**Communication method**:

- [ ] Slack/email update
- [ ] Presentation
- [ ] Written report
- [ ] Meeting

---

## 9. Documentation

### Artifacts

**Link to supporting materials**:

- Raw data: ******\_\_\_\_******
- Interview notes: ******\_\_\_\_******
- Prototype: ******\_\_\_\_******
- Screenshots: ******\_\_\_\_******
- Analytics dashboard: ******\_\_\_\_******

**Files saved in**: ******\_\_\_\_******

---

### Knowledge Sharing

**Key insights to share**:

1.
2.
3.

**Added to knowledge base**: [ ] Yes [ ] No
**Location**: ******\_\_\_\_******

**Team debrief**:

- Date: **\_\_\_\_**
- Attendees: ******\_\_\_\_******
- ## Key discussion points:
  -

---

## 10. Reflection

### Process Retrospective

**What went well?**

1.
2.
3.

**What could be improved?**

1.
2.
3.

**For next experiment, we should**:

1.
2.
3.

---

### Cost-Benefit Analysis

**Time invested**: **\_\_\_\_** hours/days
**Money invested**: $****\_****

**Value of learning**:

- [ ] Extremely valuable - Saved us from major mistake
- [ ] Very valuable - Validated important assumption
- [ ] Moderately valuable - Helpful but expected
- [ ] Somewhat valuable - Nice to know
- [ ] Not valuable - Could have assumed this

**Would you run this experiment again?**

- [ ] Yes, exactly as-is
- [ ] Yes, but with modifications: ******\_\_\_\_******
- [ ] No, would use different method: ******\_\_\_\_******
- [ ] No, not worth it

---

## Template Usage Tips

### Before Starting

**Use this template when**:

- Testing critical assumptions
- Making important product decisions
- Unsure about user behavior
- Considering significant investment

**Don't use when**:

- Answer is easily observable
- Risk of being wrong is very low
- Learning isn't worth time/cost

---

### During Experiment

**Best practices**:

- Define success criteria before seeing results
- Actively seek disconfirming evidence
- Document everything (you'll forget details)
- Don't change experiment mid-flight
- Separate data collection from analysis

---

### After Experiment

**Share learnings**:

- Failed experiments are valuable
- Document "why" not just "what"
- Update team's shared assumptions
- Build institutional knowledge

---

## Example: Completed Experiment

**Experiment Name**: Time Tracking Auto-Suggest Validation

**Assumption**: "Freelancers will accept 80%+ of auto-suggested time entries when switching projects"

**Method**: Wizard of Oz (manual suggestions disguised as automated)

**Sample**: 10 freelancers, 2 weeks

**Success Criteria**:

- Strong: > 80% acceptance rate
- Moderate: 60-80% acceptance
- Failure: < 60% acceptance

**Result**: 45% acceptance rate (Failure)

**Learning**: Users want control, not automation. They'll use suggestions as reminders but want to edit. Changed approach to "prompt + easy edit" rather than "auto-suggest + accept/reject"

**Decision**: Iterate - Change UX to treat as prompts, not automation. Test revised approach.

---

**Remember**: A well-designed experiment teaches you something valuable whether it succeeds or fails. The only failure is not learning.
