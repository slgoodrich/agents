# Usability Test Script Template

## Pre-Session Setup

**Time**: [Duration, e.g., 60 minutes]
**Participant**: [Name/ID]
**Date**: [Date]
**Facilitator**: [Your name]
**Note-taker**: [Name, if applicable]
**Recording**: [ ] Screen [ ] Audio [ ] Video

**Equipment check**:

- [ ] Recording software running
- [ ] Screen sharing working
- [ ] Audio clear
- [ ] Consent form signed
- [ ] Incentive ready

---

## Introduction (5 minutes)

**Welcome and rapport building**:

"Hi [Name], thanks so much for joining us today! How are you doing?"

[Small talk to make them comfortable]

**Set expectations**:

"Today we're testing [product/feature name] to see how it works for people like you. Just to be clear upfront: we're testing the product, not you. There are no right or wrong answers, and you can't do anything wrong. If something doesn't work or is confusing, that's exactly what we need to know so we can improve it.

I'll ask you to try a few tasks and think out loud as you go—just say whatever comes to mind. I'll be taking notes, and we're recording [screen/audio/video] so I don't miss anything. Is that okay with you?"

[Wait for consent]

**Explain think-aloud**:

"The most helpful thing you can do is think out loud. Just say what you're looking at, what you're trying to do, what you're thinking. For example, if I'm looking for a coffee shop, I might say 'Okay, I'm on the homepage, I see a search box, I'll try typing coffee shop in there...' Make sense?"

**Questions before we start**:

"Do you have any questions before we begin?"

---

## Background Questions (5 minutes)

**Demographic/Screening validation** (if not already collected):

- "Can you tell me a bit about your current role?"
- "How long have you been doing [relevant activity]?"

**Relevant experience**:

- "Have you ever used [product category] before?"
- "What tools do you currently use for [relevant task]?"
- "How often do you [relevant task]?"

**Context for test product** (if not completely new):

- "Have you heard of [product name] before?"
- "If yes: What do you know about it?"

[Adapt these questions to your specific product and research goals]

---

## Tasks Section (40 minutes)

**General instructions before first task**:

"Great, thanks for that context. Now I'm going to give you a few scenarios to try. Remember to think out loud, and let me know if anything is confusing or unclear. And again, there's no right or wrong way to do this.

If you get stuck, that's okay—just do what you'd normally do if you were actually using this on your own. You can give up on a task if you want to. And I might ask you some follow-up questions as you go, but I won't be able to help you with the tasks themselves.

Sound good?"

---

### Task 1: [Task Name]

**Scenario**:
"[Provide realistic scenario with context and motivation. Example: 'Imagine you just got an email saying your account had suspicious activity. You want to secure your account by changing your password.']"

**Success criteria** (for your notes, not shared with participant):

- [ ] Found [X feature]
- [ ] Completed [Y action]
- [ ] Successfully reached [Z outcome]

**Time started**: **\_**
**Time completed**: **\_**

**Observations while participant works**:
[Take notes on hesitations, errors, confusion, unexpected paths]

**Post-task questions**:

- "On a scale of 1 to 7, where 1 is very difficult and 7 is very easy, how would you rate that task?"
  - **Rating**: \_\_\_\_ / 7
  - "Why did you give it that rating?"

- "Was that what you expected to happen?"

- [Ask about specific observed behaviors]: - "I noticed you [behavior]—what were you thinking there?"

---

### Task 2: [Task Name]

**Scenario**:
"[Next realistic scenario]"

**Success criteria**:

- [ ] [Criterion 1]
- [ ] [Criterion 2]
- [ ] [Criterion 3]

**Time started**: **\_**
**Time completed**: **\_**

**Observations**:

**Post-task questions**:

- "How easy or difficult was that?" (1-7): \_\_\_\_
  - "Why?"

- [Specific follow-ups based on observations]

---

### Task 3: [Task Name]

**Scenario**:
"[Next realistic scenario]"

**Success criteria**:

- [ ] [Criterion 1]
- [ ] [Criterion 2]
- [ ] [Criterion 3]

**Time started**: **\_**
**Time completed**: **\_**

**Observations**:

**Post-task questions**:

- "How easy or difficult was that?" (1-7): \_\_\_\_
  - "Why?"

- [Specific follow-ups]

---

[Repeat for additional tasks, typically 4-6 tasks total]

---

## Overall Experience Questions (5 minutes)

**General impressions**:

- "Now that you've tried [product], what are your overall impressions?"

**Comparison** (if relevant):

- "How does this compare to [current solution] you mentioned using?"

**Value and intent**:

- "Based on what you've seen, would this be useful for you?"
- "Why or why not?"
- "What would make it more useful?"

**Missing expectations**:

- "Was there anything you expected to see or be able to do that you didn't find?"

**Most/least**:

- "What did you like most about it?"
- "What frustrated you the most?"

---

## Wrap-Up (5 minutes)

**Final thoughts**:
"Is there anything else you'd like to mention that we haven't covered?"

**Thank you**:
"This has been incredibly helpful. Your feedback is going to directly help us make [product] better. Thank you so much for your time."

**Logistics**:

- [Explain incentive/payment process]
- [Provide contact info if they have questions]
- [Remind about NDA if applicable]

**After participant leaves**:

- [ ] Save recording
- [ ] Note any technical issues
- [ ] Write down immediate observations while fresh
- [ ] File consent form
- [ ] Process incentive payment

---

## Notes Section

**Overall notes**:

**Patterns observed**:

**Quotes to remember**:

**Immediate recommendations**:

**Severity of issues observed**:

- Critical (P0):
- Serious (P1):
- Moderate (P2):
- Minor (P3):

---

## Customization Tips

**For early-stage prototypes**:

- Add more "what would you expect?" questions
- Focus on concept validation, not just usability
- Ask about missing features

**For comparative testing**:

- Test same tasks on competitor product first or after
- Ask explicit comparison questions

**For remote moderated tests**:

- Add tech check section at start
- Have backup communication method
- Be more explicit about think-aloud since you can't see them

**For unmoderated tests**:

- Convert to written instructions
- Pre-record instructions if using video
- Make success criteria clearer for auto-detection
- Add more post-task survey questions

---

**Remember**: This is a guide, not a rigid script. Adapt questions based on what you're learning. If something interesting comes up, probe deeper. Stay curious and neutral.
